{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06de7686",
   "metadata": {},
   "source": [
    "# Dual-sPLS\n",
    "\n",
    "Dual-sPLS implements a modified version of sPLS, providing a more intuitive way to decide how much information we want to keep with a shrinking ratio (\"replacing\" $\\lambda$ in sPLS)\n",
    "\n",
    "## Theory\n",
    "\n",
    "#### Dual Norm: Definition\n",
    "\n",
    "According to the paper:\n",
    "\n",
    "Definition 3.1: Dual Norm\n",
    "\n",
    "Let $\\Omega (.)$ be a norm on $\\mathbb{R}^p$. For any $z \\in \\mathbb{R}^p$, the associated dual norm, denoted $\\Omega^*(.)$, is defined as\n",
    "\n",
    "$$\n",
    "\\Omega^*(.) = max_w (z^Tw) \\quad s.t. \\quad \\Omega(w) = 1 \\quad (21)\n",
    "$$\n",
    "\n",
    "#### Generalizing sPLS to many other regularization\n",
    "\n",
    "Taking the expression of the regularization problem for PLS:\n",
    "\n",
    "$$\n",
    "max_w (y^TXw) \\quad s.t. \\quad ||w||_2 = 1\n",
    "$$\n",
    "\n",
    "We can generalize it to any norm\n",
    "\n",
    "$$\n",
    "max_w (y^TXw) \\quad s.t. \\quad \\Omega(w) = 1\n",
    "$$\n",
    "\n",
    "And get the expression for $\\hat{w}$\n",
    "\n",
    "$$\n",
    "\\hat{w} = argmin_w (-z^Tw) \\quad s.t. \\quad \\Omega(w) = 1\n",
    "$$\n",
    "\n",
    "The method becomes powerful because we can put any norm in $\\Omega$. For example, we can have a lasso penalization as in sPLS and find the same result, but also combination of norms, with for example the first proposition made by the paper: pseudo-lasso:\n",
    "\n",
    "$$\n",
    "\\Omega(w) = \\lambda ||w||_1 + ||w||_2\n",
    "$$\n",
    "\n",
    "which will be used to illustrate the method in this notebook.\n",
    "\n",
    "We can then apply our Lagrangian method:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = -z^Tw + \\mu(\\Omega(w) - 1) \\quad ; \\quad \\mu > 0 ^*\n",
    "$$\n",
    "\n",
    "\\* Usually not the case for an equality constraint, but here we want the constraint to be active\n",
    "\n",
    "With a very similar reasoning than we have in sPSL (see sPLS.ipynb), we get:\n",
    "\n",
    "$$\n",
    "\\nabla \\Omega(w) = \\frac{w}{\\mu}\n",
    "$$\n",
    "\n",
    "to solve the non-differentiability issues \n",
    "\n",
    "($u_i = +1 \\quad if \\quad  w_i > 0$; $u_i = -1 \\quad if \\quad  w_i < 0$; $u_i \\in [-1, +1] \\quad if \\quad  w_i = 0$)\n",
    "\n",
    "Which gives us the same soft-thresholding than seen in sPLS. \n",
    "\n",
    "___\n",
    "\n",
    "Let's find the right expression for pseudo-lasso as we will need it now:\n",
    "\n",
    "$$\n",
    "\\nabla \\Omega(w) = \\lambda \\delta + \\frac{w}{||w||_2}\n",
    "$$\n",
    "\n",
    "where $\\delta = sign(w) = sign(z)$, see sPLS\n",
    "\n",
    "$$\n",
    "\\nabla \\Omega(w) = \\frac{z}{\\mu} = \\lambda \\delta + \\frac{w}{||w||_2} \\iff \\frac{w}{||w||_2} = \\frac{z}{\\mu} - \\lambda \\delta \\Rightarrow \\frac{w_p}{||w||_2} = \\frac{1}{\\mu}\\delta_p(|z_p| - \\nu)_+\n",
    "$$\n",
    "\n",
    "where $\\nu = \\lambda \\mu$\n",
    "\n",
    "Then, we can decide to keep $\\xi \\%$ of the most important values, and find the right value for $\\nu$ by computing the quantile in $z$ for $\\xi$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"assets/dualsplsfig1.png\" alt=\"Description\" width=\"400\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "But we cannot simply keep zeros in $z$ after the soft-thresholding; we still need to respect $\\Omega(w) = 1$. \"To guarantee the unit norm property for $w$, we set $\\mu = ||z_\\nu||_2$ where $z_\\nu$ is the vector of coordinates $\\delta_p(|z_p| - \\nu)_+$ for $p\\in \\{ 1, ..., P\\}$. Consequently,\n",
    "\n",
    "$$\n",
    "w = \\frac{\\mu}{\\nu||z_\\nu||_1 + ||z_\\nu||_2^2}z_\\nu\n",
    "$$\n",
    "\n",
    "The rationale behind constrainting the direction $w$ instead of the regression coeﬃcients $\\hat{\\beta}$ is their collinearity. Indeed, the estimator writes\n",
    "$\\hat{\\beta} = W(T^TT)^{−1}T^Ty$. Being collinear, soft-thresholding $w$ performs a variable selection at the same location in $\\hat{\\beta}$ coordinates.\"\n",
    "\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69067579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Utils:\n",
    "\n",
    "def soft_thresholding(z, nu):\n",
    "    \"\"\"\n",
    "    Modified to use nu\n",
    "    \"\"\"\n",
    "    sign_z = np.sign(z)\n",
    "    abs_z_shifted = np.maximum(np.abs(z) - nu, 0)\n",
    "    z_nu = sign_z * abs_z_shifted\n",
    "    \n",
    "    # On remet le signe et on retourne\n",
    "    return z_nu\n",
    "\n",
    "def center_matrix(M):\n",
    "    \"\"\"Center matrix computing means on columns\n",
    "\n",
    "    Args:\n",
    "        M (np.array): 2D-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: centered 2D-matrix\n",
    "    \"\"\"\n",
    "    means = np.mean(M, axis=0)\n",
    "    return M - means, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a795263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_dual_spls_lasso(E, F, ppnu=0.8):\n",
    "    \"\"\"\n",
    "    Modified version of the previous base_sPLS (and base_PLS) functions\n",
    "\n",
    "    ppnu: Xi in the explanations, percentage\n",
    "    \"\"\"\n",
    "    F = F.reshape(-1, 1)\n",
    "\n",
    "    z = np.transpose(E) @ F  \n",
    "\n",
    "    #### Modification 1: compute the adaptative nu\n",
    "    nu = np.quantile(np.abs(z), ppnu)\n",
    "\n",
    "    #### Modification 2: different soft-thresholding\n",
    "    z_nu = soft_thresholding(z, nu)\n",
    "\n",
    "    #### Modification 3: Find paramters \n",
    "    z_nu_1 = np.linalg.norm(z_nu, 1)\n",
    "    z_nu_2 = np.linalg.norm(z_nu, 2)\n",
    "    \n",
    "    mu=z_nu_2 \n",
    "    _lambda = nu/mu\n",
    "\n",
    "    #### Compute w\n",
    "    scaling_factor = mu / (nu * z_nu_1 + mu**2) # Scaling factor, see theory\n",
    "    w = scaling_factor*z_nu\n",
    "\n",
    "    #### Compute t, same as sPLS\n",
    "    t = E @ w\n",
    "\n",
    "    # + Modification 4: Normalize t instead of w and c \n",
    "    norm_t = np.linalg.norm(t)\n",
    "    if norm_t > 1e-10:\n",
    "        t = t / norm_t\n",
    "    else:\n",
    "        t = np.zeros_like(t)\n",
    "\n",
    "    # c is not used by the R package\n",
    "\n",
    "    return w, t, _lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219a7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_spls_lasso(X, y, n_components=3, ppnu=0.8):\n",
    "    #### Center data\n",
    "    E, F = X.copy(), y.copy()\n",
    "    E, E_mean = center_matrix(E)\n",
    "    F, F_mean = center_matrix(F)\n",
    "\n",
    "    if F.ndim == 1:\n",
    "        F = F.reshape(-1, 1)\n",
    "\n",
    "    #### Initializations\n",
    "    N, p = X.shape[0], X.shape[1] # nbr of observations, nbr of variables\n",
    "    WW = np.zeros((p, n_components)) # W: X weights\n",
    "    TT = np.zeros((N, n_components)) # T: X scores\n",
    "    listeLambda = np.zeros((n_components))\n",
    "    Bhat = np.zeros((p, n_components)) # Matrix to store Beta for each n_components step\n",
    "    intercept = np.zeros(n_components)\n",
    "    RES = np.zeros((N, n_components)) \n",
    "    zerovar = np.zeros(n_components, dtype=int)\n",
    "    YY_pred = np.zeros((N, n_components)) # Fitted values\n",
    "    ind_diff0 = {} \n",
    "\n",
    "    Ec = E.copy()\n",
    "\n",
    "    for k in range(n_components):\n",
    "        # Step 1: base dual-spls:\n",
    "        w, t, _lambda = base_dual_spls_lasso(E, F, ppnu=ppnu)\n",
    "\n",
    "        # Store results\n",
    "        WW[:, k], TT[:, k] = w.reshape(-1), t.reshape(-1)\n",
    "        listeLambda[k] = _lambda\n",
    "\n",
    "        # Deflate E: \n",
    "        E = E - t @ (t.T @ E)\n",
    "\n",
    "\n",
    "        W_k = WW[:, :k+1]\n",
    "        T_k = TT[:, :k+1]\n",
    "        L = np.transpose(T_k) @ Ec @ W_k # \"backsolve\"\n",
    "        L = np.triu(L) # \"R[row>col]=0\"\n",
    "\n",
    "        try:\n",
    "            L_inv = np.linalg.inv(L)\n",
    "        except:\n",
    "            L_inv = np.linalg.pinv(L)\n",
    "        \n",
    "        bk = W_k @ L_inv @ T_k.T @ F\n",
    "        bk_flat = bk.flatten() \n",
    "        Bhat[:, k] = bk_flat\n",
    "\n",
    "        intercept[k] = (F_mean - E_mean @ bk).item()\n",
    "\n",
    "        # Zero variables : count almost zero coefficients\n",
    "        is_zero = np.isclose(bk_flat, 0)\n",
    "        zerovar[k] = np.sum(is_zero)\n",
    "\n",
    "        # non-zero indices \n",
    "        indices_non_zero = np.where(~is_zero)[0]\n",
    "        ind_diff0[f\"in.diff0_{k+1}\"] = indices_non_zero.tolist()\n",
    "\n",
    "        # Predictions (Fitted Values) \n",
    "        # Y_hat = X * beta + intercept\n",
    "        pred_k = (X @ bk_flat) + intercept[k]\n",
    "        YY_pred[:, k] = pred_k\n",
    "\n",
    "        # Residuals\n",
    "        RES[:, k] = y.flatten() - pred_k\n",
    "\n",
    "    return {\n",
    "        \"Xmean\": E_mean,\n",
    "        \"scores\": TT,\n",
    "        \"loadings\": WW,\n",
    "        \"Bhat\": Bhat,\n",
    "        \"intercept\": intercept,\n",
    "        \"fitted_values\": YY_pred,\n",
    "        \"residuals\": RES,\n",
    "        \"lambda\": listeLambda,\n",
    "        \"zerovar\": zerovar,\n",
    "        \"ind_diff0\": ind_diff0,\n",
    "        \"type\": \"lasso\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0c060",
   "metadata": {},
   "source": [
    "## Quick test\n",
    "\n",
    "*author: @gemini*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54da9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Initialization: n=50, p=100, n_components=3\n",
      ">> Dimension validation: SUCCESS\n",
      ">> Number of zero coefficients per component: [90 82 76]\n",
      ">> Functional validation: Lasso successfully performed variable selection.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_dual_spls_routine():\n",
    "    # 1. Simulation Setup\n",
    "    n_obs = 50\n",
    "    n_vars = 100\n",
    "    n_comp = 3\n",
    "\n",
    "    print(f\"Test Initialization: n={n_obs}, p={n_vars}, n_components={n_comp}\")\n",
    "\n",
    "    # 2. Random Data Generation\n",
    "    np.random.seed(42)\n",
    "    X_sim = np.random.normal(0, 1, size=(n_obs, n_vars))\n",
    "\n",
    "    # Create a target vector correlated to strictly the first 5 variables\n",
    "    beta_true = np.zeros(n_vars)\n",
    "    beta_true[:5] = [2.5, -1.5, 3.0, 0.5, -2.0]\n",
    "    y_sim = X_sim @ beta_true + np.random.normal(0, 0.1, size=n_obs)\n",
    "\n",
    "    # 3. Algorithm Execution\n",
    "    try:\n",
    "        # High ppnu to enforce sparsity (variable selection)\n",
    "        res = dual_spls_lasso(X_sim, y_sim, n_components=n_comp, ppnu=0.9)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical execution error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Dimensionality Checks (Assertions)\n",
    "    try:\n",
    "        # Verify return type\n",
    "        assert isinstance(res, dict), \"Return format must be a dictionary.\"\n",
    "\n",
    "        # Verify Beta coefficients (p, n_comp)\n",
    "        assert res['Bhat'].shape == (n_vars, n_comp), \\\n",
    "            f\"Incorrect dimension for Bhat. Expected {(n_vars, n_comp)}, received {res['Bhat'].shape}\"\n",
    "\n",
    "        # Verify Scores (n, n_comp)\n",
    "        assert res['scores'].shape == (n_obs, n_comp), \\\n",
    "            f\"Incorrect dimension for scores. Expected {(n_obs, n_comp)}, received {res['scores'].shape}\"\n",
    "\n",
    "        # Verify Fitted Values (n, n_comp)\n",
    "        assert res['fitted_values'].shape == (n_obs, n_comp), \\\n",
    "            f\"Incorrect dimension for fitted_values. Expected {(n_obs, n_comp)}, received {res['fitted_values'].shape}\"\n",
    "\n",
    "        print(\">> Dimension validation: SUCCESS\")\n",
    "\n",
    "        # 5. Functional Check (Sparsity)\n",
    "        nb_zeros = res['zerovar']\n",
    "        print(f\">> Number of zero coefficients per component: {nb_zeros}\")\n",
    "\n",
    "        if np.any(nb_zeros > 0):\n",
    "            print(\">> Functional validation: Lasso successfully performed variable selection.\")\n",
    "        else:\n",
    "            print(\">> Warning: No variables selected (Check ppnu or normalization).\")\n",
    "\n",
    "    except AssertionError as error:\n",
    "        print(f\"Validation test failure: {error}\")\n",
    "\n",
    "# Run the routine\n",
    "test_dual_spls_routine()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
