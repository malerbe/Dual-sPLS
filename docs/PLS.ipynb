{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac9fcde",
   "metadata": {},
   "source": [
    "# PLS (Partial Least Square / Projection to Latent Structures)\n",
    "\n",
    "## 1. Introduction to PLS\n",
    "\n",
    "PLS is a method to project a high-dimensional feature space into a lower-dimensional space while preserving as much relevant information as possible.\n",
    "\n",
    "Unlike Principal Component Analysis (PCA) which is also a famous dimensionality reduction method, PLS not only look at the features $X$ but also the label $y$. \n",
    "\n",
    "Thus, PLS tries to make a feature extraction that explains both the variance within the features $X$ (like PCA) **AND** that are correlated with the labels $y$. \n",
    "\n",
    "*Why use PLS ?*: \"The goal of PLS regression is to predict $Y$ from $X$ and to describe their common structure. When $Y$ is a vector and $X$ is full rank, this goal could be accomplished using ordinary multiple regression. When the number of predictors is large compared to the number of observations, $X$ is likely to be singular and the regression approach is no longer feasible (i.e., because of multicollinearity). Several approaches have been developed to cope with this problem. Several approaches have been developed to cope with this problem. One approach is to eliminate some predictors (e.g., using step-wise methods) another one, called principal component regression, is to perform a Principal Component Analysis (PCA) of the $X$ matrix and then use the principal components of $X$ as regressors on $Y$. The orthogonality of the principal components eliminates the multicolinearity problem. But, the problem of choosing an optimum subset of predictors remains. A possible strategy is to keep only a few of the first components. But they are chosen to explain $X$ rather than $Y$, and so, nothing guarantees that the principal components, which “explain” X, are relevant for $Y$.\" [1]\n",
    "\n",
    "\n",
    "## 2. Step-by-step implementation\n",
    "### 2.0 Data importation\n",
    "\n",
    "To test our code, we will import a dummy dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e02e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171105a",
   "metadata": {},
   "source": [
    "### 2.1 Data preparation\n",
    "\n",
    "\"The properties of pls regression can be analyzed from a sketch of the original algorithm. The first step is to create two matrices: $E=X$ and $F=Y$. These matrices are then column centered and normalized (i.e., transformed into Z-scores).\" [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e3195c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def z_score_standardize(M):\n",
    "    \"\"\"Standardize a dataset matrix computing means and stds on columns\n",
    "\n",
    "    Args:\n",
    "        M (np.array): 2D-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: standardized 2D-matrix\n",
    "    \"\"\"\n",
    "    means = np.mean(M, axis=0)\n",
    "    stds = np.std(M, axis=0)\n",
    "\n",
    "    return (M - means) / stds\n",
    "\n",
    "E, F = z_score_standardize(X.copy()), z_score_standardize(y.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35851a",
   "metadata": {},
   "source": [
    "### 2.2 Base algorithm\n",
    "\n",
    "The goal of the algorithm is to find the best straight line (of direction $w$) on which the projected points ($t$, called \"scores\")from $X$ maximize the covariance with $y$. \n",
    "\n",
    "\"Before starting the iteration process, the vector $\\textbf{u}$ is initialized with random values.\" [1]\n",
    "\n",
    "#### Step 1: Estimate $X$ weights\n",
    "**$w$ (the weight vector)** is the variable the algorithm optimizes. The optimization problem is defined as:\n",
    "\n",
    "$$\n",
    "\\max_w \\left( \\text{Cov}(Xw, y) \\right)^2 \\quad \\text{subject to} \\quad \\|w\\|_2 = 1\n",
    "$$\n",
    "\n",
    "This is equivalent to maximizing the dot product:\n",
    "\n",
    "$$\n",
    "\\max_w \\langle Xw, y \\rangle = \\max_w \\left( w^T X^T y \\right)\n",
    "$$\n",
    "\n",
    "We solve this using the **Lagrangian**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, \\lambda) = w^T X^T y - \\lambda(w^T w - 1)\n",
    "$$\n",
    "\n",
    "Computing the derivative with respect to $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = X^T y - 2\\lambda w = 0\n",
    "$$\n",
    "\n",
    "Which implies:\n",
    "\n",
    "$$\n",
    "w = \\frac{1}{2\\lambda} X^T y \\quad \\implies \\quad w \\propto X^T y\n",
    "$$\n",
    "\n",
    "**However**, this analytical solution  $w \\propto X^Ty$  is strictly sufficient only in the PLS1 case (where $y$  is a single column vector). In this specific scenario, the relationship is linear and direct, and convergence is immediate.\n",
    "\n",
    "In the general PLS2 case, where $y$ is a matrix with multiple columns, he target is not a fixed vector $y$ but a latent vector $u$ $\\in \\mathbb{R}^{n \\times 1}$, which represents a linear combination of the columns of $y$.\n",
    "\n",
    "Since $u$ depends on the weights of $y$, which in turn depend on the scores of $X$, we face a circular dependency:\n",
    "\n",
    "$w \\propto X^Tu$ and $u \\propto Fc(w)$\n",
    "\n",
    "*Therefore, the algorithm cannot solve for $w$ in a single step. Instead, it employs an iterative procedure (NIPALS). By alternating projections between the $X$  space and the $y$  space, the algorithm implements the Power Method. This iterative process is mathematically guaranteed to converge to the eigenvector associated with the largest eigenvalue of the cross-covariance matrix  $X^TYY^TX$, thus identifying the direction of maximum shared variance.*\n",
    "\n",
    "#### Step 2: Estimate $t$ ($X$ factor scores)\n",
    "\n",
    "$$\n",
    "t \\propto Ew\n",
    "$$\n",
    "\n",
    "This step is simply the computation of the projected coordinates of the points of $X$ ($\\mathbb{R}^p \\to \\mathbb{R}^1$)\n",
    "\n",
    "#### Step 3: Estimate $c$ ($y$ weights)\n",
    "\n",
    "$$\n",
    "c \\propto F^Tt\n",
    "$$\n",
    "\n",
    "This step corresponds to the computation of the covariance between $y$ and the scores $t$. It tells us how each column of $y$ is explained by the current score $t$ (i.e. by the latent representation made).\n",
    "\n",
    "#### Step 4: Estimate $u$ ($y$ scores)\n",
    "\n",
    "$$\n",
    "u = Fc \\iff \\forall i, u_i = \\sum_{j=1}^q F_{ij}c_j\n",
    "$$\n",
    "\n",
    "At this step, we compute every $u_i$ which represents the linear combination of the labels $y$, weighted by their current importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ff523b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_PLS(E, F):\n",
    "    if F.ndim == 1:\n",
    "        \"\"\"In the case of a $y$ with a single column, it is not useful to use the algorithm\n",
    "        as we have an analytic answer to the problem.\"\"\"   \n",
    "        F = F.reshape(-1, 1)\n",
    "        w = np.transpose(E) @ F\n",
    "        w = w/np.linalg.norm(w, ord=2) # normalization to gauarantee ||w|| = 1\n",
    "\n",
    "        t = E @ w\n",
    "\n",
    "        c = np.transpose(F) @ t\n",
    "        c = c / np.linalg.norm(c) # normalization\n",
    "    else: \n",
    "        \"\"\"Else, we need to use the PLS2 algorithm to estimate a solution. \n",
    "        \"\"\"\n",
    "\n",
    "        u = np.random.rand(E.shape[0]) # Initialize u with random values\n",
    "        u = u.reshape(-1, 1) # Reshape for coherence in matrixes shapes\n",
    "\n",
    "        for k in range(10): # To be changed with a real stop condition\n",
    "            # Step 1: estimate X weights\n",
    "            w = np.transpose(E) @ u\n",
    "            w = w/np.linalg.norm(w, ord=2) # normalization\n",
    "\n",
    "            # Step 2: Estimate t, the factor scores\n",
    "            t = E @ w\n",
    "\n",
    "            # Step 3: Estimate c (y weights)\n",
    "            c = np.transpose(F) @ t\n",
    "            c = c / np.linalg.norm(c) # normalization\n",
    "\n",
    "            # Step 4: Estimate $y$ scores\n",
    "            u = F @ c\n",
    "        \n",
    "    return w, t, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2b550",
   "metadata": {},
   "source": [
    "### 2.3 Full algorithm\n",
    "\n",
    "The idea is now to adapt the code so that it can be used to extract multiple components, not just the main one. \n",
    "\n",
    "#### Step 1: Apply the base PLS algorithm seen previously\n",
    "\n",
    "#### Step 2: Compute $p$ \n",
    "\n",
    "We can explain this step by considering the first step as an encoding step. $w$ tells us how to encode our data in a more compact way while keeping as much information as possible. $p$ is what we need to \"decode\" the encoded information. It's the \"opposite\". \n",
    "\n",
    "Rigorously, we want to find $p$ to reconstruct $E$*:\n",
    "\n",
    "$E \\approx tp^T$\n",
    "\n",
    "We thus have this optimisation problem**:\n",
    "\n",
    "$$\n",
    "min_p ||E - tp^T||_F^2\n",
    "$$\n",
    "\n",
    "\"What is the $p$ minimizing the reconstruction error ?\" \n",
    "\n",
    "$\\Rightarrow p^T = (t^Tt)^{-1}t^TE \\Rightarrow p = \\frac{E^Tt}{t^Tt}$\n",
    "\n",
    "<small>\n",
    "* We cannot really compute the pseudo-invert of $w$ and simply have $t = Ew \\Rightarrow E = tw^{*}$, this would cause orthogonality problems. \n",
    "\n",
    "\\*\\* Frobenius norm allows an easy derivation. Minimizing the Frobenius norm is equivalent to performing a standard Least Squares Linear Regression for each variable (column) of $X$ simultaneously\n",
    "</small>\n",
    "\n",
    "#### Step 3: Deflation\n",
    "\n",
    "This step corresponds to the substraction of the information explained by the component extracted:\n",
    "\n",
    "$$\n",
    "E_{new} = E_{old} - tp^T\n",
    "$$\n",
    "\n",
    "where $tp^T$, as explained in the Step 2, is the \"simplified\" image of $E$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a658852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta': array([[-0.00671583],\n",
       "        [-0.15736633],\n",
       "        [ 0.32931638],\n",
       "        [ 0.19719245],\n",
       "        [-0.02852215],\n",
       "        [-0.07935239],\n",
       "        [-0.12684228],\n",
       "        [ 0.07462938],\n",
       "        [ 0.27045501],\n",
       "        [ 0.0676613 ]]),\n",
       " 'rotation_matrix': array([[-0.15555647, -0.15918207,  0.18952191],\n",
       "        [-0.0356518 , -0.44194866,  0.60701945],\n",
       "        [-0.4855326 ,  0.52526487, -0.19423333],\n",
       "        [-0.36551068,  0.26588359,  0.04778442],\n",
       "        [-0.17553722, -0.37878337, -0.33914666],\n",
       "        [-0.14410209, -0.49482996, -0.24268864],\n",
       "        [ 0.32685311, -0.1159892 , -0.25221622],\n",
       "        [-0.35637967, -0.15492979,  0.00229313],\n",
       "        [-0.46850436,  0.30070811, -0.37719118],\n",
       "        [-0.31666494, -0.02393712,  0.47622418]]),\n",
       " 'training_scores': array([[-1.04908728,  1.16869634,  0.95697385],\n",
       "        [ 2.95072957, -0.46628332, -1.01320231],\n",
       "        [-0.51371155,  0.54017185,  1.18816627],\n",
       "        ...,\n",
       "        [ 0.30475429, -0.45524956,  1.89102775],\n",
       "        [-0.99442812,  0.99415356, -1.56291516],\n",
       "        [ 2.67146226, -1.97421563, -2.13284068]], shape=(442, 3)),\n",
       " 'loadings_P': array([[-0.21477724, -0.17065959,  0.17169428],\n",
       "        [-0.1667701 , -0.34141273,  0.64102434],\n",
       "        [-0.37406285,  0.36709003, -0.10987651],\n",
       "        [-0.31914914,  0.18279453,  0.12065556],\n",
       "        [-0.30010019, -0.52105389, -0.50638131],\n",
       "        [-0.29966534, -0.61252502, -0.34502726],\n",
       "        [ 0.32069741, -0.08139668, -0.33992956],\n",
       "        [-0.43124557, -0.26503662, -0.01557792],\n",
       "        [-0.42070306,  0.09722136, -0.20428464],\n",
       "        [-0.35021798, -0.01357139,  0.27930684]]),\n",
       " 'weights_W': array([[-0.15555647, -0.20427827,  0.15499505],\n",
       "        [-0.0356518 , -0.4522842 ,  0.51116004],\n",
       "        [-0.4855326 ,  0.38450778, -0.0803025 ],\n",
       "        [-0.36551068,  0.15992115,  0.10545502],\n",
       "        [-0.17553722, -0.42967204, -0.42130542],\n",
       "        [-0.14410209, -0.53660551, -0.35001809],\n",
       "        [ 0.32685311, -0.02123368, -0.27737447],\n",
       "        [-0.35637967, -0.25824513, -0.0313114 ],\n",
       "        [-0.46850436,  0.16488755, -0.31196708],\n",
       "        [-0.31666494, -0.11573906,  0.47103218]])}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PLS(E, F, n_components=3):\n",
    "    E = E.copy()\n",
    "    F = F.copy()\n",
    "\n",
    "    if F.ndim == 1:\n",
    "        F = F.reshape(-1, 1)\n",
    "\n",
    "    # Initializations\n",
    "    WW = np.zeros((E.shape[1], n_components)) # W: X weights\n",
    "    TT = np.zeros((E.shape[0], n_components)) # T: X scores\n",
    "    PP = np.zeros((E.shape[1], n_components)) # P: X loadings\n",
    "    QQ = np.zeros((F.shape[1], n_components)) # Q: y loadings\n",
    "\n",
    "    for k in range(n_components):\n",
    "        # Step 1: Previously coded base PLS algorithm\n",
    "        w, t, c = base_PLS(E, F)\n",
    "        \n",
    "        # Store results\n",
    "        WW[:, k], TT[:, k] = w.reshape(-1), t.reshape(-1)\n",
    "\n",
    "        # Step 2: Compute p\n",
    "        p = (E.T @ t) / (t.T @ t)\n",
    "        PP[:, k] = p.reshape(-1)\n",
    "        \n",
    "        q = (F.T @ t) / (t.T @ t) \n",
    "        QQ[:, k] = q.reshape(-1)\n",
    "\n",
    "        # Deflation\n",
    "        E = E - t @ p.T \n",
    "        F = F - t @ c.T \n",
    "\n",
    "    # Compute Beta\n",
    "    rotation_matrix = WW @ np.linalg.pinv(PP.T @ WW)\n",
    "    beta = rotation_matrix @ QQ.T\n",
    "\n",
    "    return {\n",
    "        \"beta\": beta,               \n",
    "        \"rotation_matrix\": rotation_matrix, # To extract features T of a new dataset X\n",
    "        \"training_scores\": TT,      # features T from train set (for analysis)\n",
    "        \"loadings_P\": PP,     \n",
    "        \"weights_W\": WW             \n",
    "    }\n",
    "\n",
    "PLS(E, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1a0de",
   "metadata": {},
   "source": [
    "## 3. Comparison to skikit-learn\n",
    "\n",
    "To make sure that the previous implementation is right, we can make a simple linear regression on some data using both the scikit-learn library and our implementation. We will check by comparing how the linear regression works after extracting the features using PLS in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8c3866c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Scikit-Learn Performance: \n",
      "Mean Squared Error: 0.5338921572218949\n",
      "Samples from T_train and T_test:\n",
      "[[ 1.78007629  0.8767078   0.24507304]\n",
      " [ 0.58117233  0.88484495  1.05141395]\n",
      " [ 1.22494241 -1.93462658 -0.80106168]\n",
      " [-2.96148717 -0.85667361 -1.33923066]\n",
      " [-3.35428272 -0.03443725 -0.31127994]]\n",
      "[[ 1.38452991  2.16582274 -1.9756764 ]\n",
      " [ 0.28502179 -1.34667062 -0.30881613]\n",
      " [ 1.12752396  1.66927976 -0.84066678]\n",
      " [ 5.02256128 -0.76748104 -0.57587736]\n",
      " [ 0.52840036  1.57810109  0.24706615]]\n",
      "######################################################################\n",
      "Implementation from-scratch Performance: \n",
      "Mean Squared Error: 0.5338921572218946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Configuration\n",
    "n_components = 3\n",
    "\n",
    "####################\n",
    "# Scikit-learn:\n",
    "####################\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test = z_score_standardize(X_train.copy()), z_score_standardize(X_test.copy())\n",
    "y_train, y_test = z_score_standardize(y_train.copy()), z_score_standardize(y_test.copy())\n",
    "\n",
    "# Initialize PLS model with the desired number of components\n",
    "pls_model = PLSRegression(n_components=n_components)\n",
    "\n",
    "# Fit the model on the training data\n",
    "pls_model.fit(X_train, y_train)\n",
    "\n",
    "T_train = pls_model.x_scores_ \n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = pls_model.predict(X_test)\n",
    "T_test = pls_model.transform(X_test) \n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"#\"*70)\n",
    "print(\"Scikit-Learn Performance: \")\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(\"Samples from T_train and T_test:\")\n",
    "print(T_train[:5])\n",
    "print(T_test[:5])\n",
    "\n",
    "####################\n",
    "# Implementation from-scratch:\n",
    "####################\n",
    "pls_results = PLS(X_train, y_train, n_components=n_components)\n",
    "y_pred_scratch = X_test @ pls_results[\"beta\"]\n",
    "mse_scratch = mean_squared_error(y_test, y_pred_scratch)\n",
    "\n",
    "print(\"#\"*70)\n",
    "print(\"Implementation from-scratch Performance: \")\n",
    "print(f\"Mean Squared Error: {mse_scratch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf60f18",
   "metadata": {},
   "source": [
    "Results are very close, which means that most probably, our implementation works fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f019e80",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] \"Partial Least Squares (PLS) Regression\" Hervé Abdi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
